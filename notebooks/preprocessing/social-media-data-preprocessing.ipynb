{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data Preprocessing: Social Media Data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import warnings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "np.random.seed(1040)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "DATA_DIR = Path(\"../../data\")\n",
    "input_dir = DATA_DIR / \"raw\" / \"social-media-data\"\n",
    "stream_1_path = input_dir / \"Stream1.xlsx\"\n",
    "stream_2_path = input_dir / \"Stream2.xlsx\"\n",
    "stream_3_path = input_dir / \"Stream3.xlsx\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# this is ok and will not cause problems\n",
    "warnings.filterwarnings(\"ignore\", message=\"Workbook contains no default style, apply openpyxl's default\")\n",
    "\n",
    "stream_1_data = pd.read_excel(stream_2_path, engine=\"openpyxl\") # stream 1 contains the chronologically second part\n",
    "stream_2_data = pd.read_excel(stream_1_path, engine=\"openpyxl\") # stream 2 contains the chronologically first part\n",
    "stream_3_data = pd.read_excel(stream_3_path, engine=\"openpyxl\")\n",
    "\n",
    "data = pd.concat([stream_1_data, stream_2_data, stream_3_data], ignore_index=True);"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## First Look"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The data contains a great number of different attributes for each observation.\n",
    "therefore we start by looking at the attributes to get an idea of what to keep and what to get rid of."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First we convert all attribute-names to lowercase and replace white-spaces to underscores to make things simpler"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data.columns = [c.replace(' ', '_').lower() for c in stream_1_data.columns]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Important:\n",
    "\n",
    "- post_id (ID)\n",
    "- Sound Bite Text (main text corpus)\n",
    "- Published Date (GMT+01:00) London (used to create dynamic embeddings)\n",
    "- Sentiment (used for extrinsic evaluation)\n",
    "\n",
    "In the following I focus first on these attributes to keep things clear and simple"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "important_attributes = [\"post_id\", \"sound_bite_text\", \"published_date_(gmt+01:00)_london\", \"sentiment\"]\n",
    "\n",
    "data = pd.DataFrame(data, columns=important_attributes)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For simplicity's sake I chose to rename the attributes to a more readable and manageable form"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data.rename(columns={\"source_type\": \"source\", \"sound_bite_text\":\"raw_text\", \"published_date_(gmt+01:00)_london\": \"date\", \"post_id\":\"id\"}, inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let us look at how many values are actually there for the selected attributes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# number of observations\n",
    "n = len(data)\n",
    "\n",
    "# Display relative counts of missing values\n",
    "data.isnull().sum().divide(n).sort_values(ascending=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Both the date and the text have almost no missing values, which is the main thing.\n",
    "The attribute sentiment will only be used for a part of the evaluation of the embeddings and is therefore not as important.\n",
    "I therefore decide to go for the following strategy:\n",
    "\n",
    "Remove observations:\n",
    "\n",
    "- with missing date\n",
    "- with missing text\n",
    "\n",
    "Keep observations:\n",
    "- with missing sentiment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let us now describe the key characteristics of our (remaining) data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data.dtypes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The datatypes are mostly as we would like.\n",
    "We only convert the date attribute from object to date, since we are working with a time series."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data['date'].isnull().sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Given that temporal word embeddings heavily rely on dates, we consider the date to be crucial. However, out of the 24 tweets available, some lack a date, so I opt to eliminate those observations from the dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data.dropna(subset=['date'], inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data['date'] =  pd.to_datetime(data['date'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since I focus on the temporal change of words, I chose to sort the observations by date because that makes a manual inspection later on more convenient"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data.sort_values('date', inplace=True);"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The text of the tweets is the main source of information, lets look how many missing values we encounter here"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "null_texts = data['raw_text'].isnull().sum()\n",
    "empty_texts = data[data['raw_text'].str.len() < 2].count().iloc[0]\n",
    "print(f\"Obsersations with no text: {null_texts}\")\n",
    "print(f\"Obsersations with empty text: {empty_texts}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since it is only one observation we can safely remove it to prevent it from causing errors later on."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data.dropna(subset=['raw_text'], inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now lets take a look at the different attributes. Since the task at hand is a sentiment analysis, we focus on this attribute first"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data[\"sentiment\"].unique()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "So our target is to predict the sentiment from the text (sound_bite_text).\n",
    "The sentiment is either:\n",
    "\n",
    "- Positive\n",
    "- Negative\n",
    "- Neutral\n",
    "- Mixed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# Get the range of dates\n",
    "period = (data['date'].min(), data['date'].max())\n",
    "\n",
    "# Format the output\n",
    "formatted_range = tuple(date.strftime(\"%Y-%m-%d\") for date in period)\n",
    "print(\"Period of time:\", formatted_range)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Convert to lowercase"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data[\"text\"] = data[\"raw_text\"].str.lower()\n",
    "\n",
    "# rearrange columns\n",
    "data = data[['id', 'text', \"raw_text\", 'date', 'sentiment']]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Remove Unicode Characters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Eliminate the punctuation, URL, and @"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "\n",
    "    # Removes all of them\n",
    "    text = re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", text)\n",
    "\n",
    "    return text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data[\"text\"] = data[\"text\"].apply(clean_text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Remove Stopwords"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "import nltk\n",
    "# nltk.download('punkt')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.tokenize import word_tokenize\n",
    "\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
    "    return \" \".join(filtered_text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data[\"text\"] = data[\"text\"].apply(remove_stopwords)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4. Stemming (not used)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Good for extrinsic evaluation but bad for visualization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def perform_stemming(text):\n",
    "    stemmer = SnowballStemmer(language = \"english\")\n",
    "    word_tokens = word_tokenize(text)\n",
    "    stemmed_text = [stemmer.stem(word) for word in word_tokens]\n",
    "    return \" \".join(stemmed_text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Note: No Stemming is done at the moment**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# data[\"text\"] = data[\"text\"].apply(perform_stemming)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check if we inadvertently created some Na, Null Values in our (processed) text column"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data[\"text\"].isna().sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "data[\"text\"].isnull().sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def save_to_csv(data, splits: list, sub_dir: str):\n",
    "\n",
    "    output_dir = DATA_DIR / \"processed\" / \"social-media-data\" / sub_dir\n",
    "    # Create output directory if it doesn't exist\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    # range of the observations\n",
    "    lower = data[\"date\"].min()\n",
    "    upper = data[\"date\"].max()\n",
    "\n",
    "    for split in splits[1:]:\n",
    "        split_df = data[(lower <= data['date']) & (data['date'] < split)]\n",
    "        split_filename = output_dir / f\"{lower.strftime('%d_%b')}_to_{split.strftime('%d_%b')}.csv\"\n",
    "        print(f\"{lower.strftime('%d_%b')}_to_{split.strftime('%d_%b')}.csv\")\n",
    "        # Save the filtered data to csv, overwrite if exists\n",
    "        split_df.to_csv(split_filename, index=False, mode='w')\n",
    "        # Update the lower date for the next iteration\n",
    "        lower = split\n",
    "\n",
    "    # take care of second half of the last split\n",
    "    split_df = data[(lower <= data['date']) & (data['date'] <= upper)]\n",
    "    split_filename = output_dir / f\"{lower.strftime('%d_%b')}_to_{upper.strftime('%d_%b')}.csv\"\n",
    "    print(str(split_filename).split(f\"{sub_dir}/\")[-1])\n",
    "    split_df.to_csv(split_filename, index=False, mode='w')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Save whole corpus as one"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "output_dir = DATA_DIR / \"processed\" / \"social-media-data\"\n",
    "data.to_csv(output_dir / \"corpus_all.csv\", index=False, mode='w')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Splitting"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "cleaned_data_path = output_dir / \"corpus_all.csv\"\n",
    "data = pd.read_csv(cleaned_data_path)\n",
    "\n",
    "# Remove text null texts which occurred due to preprocessing and saving to csv\n",
    "data.dropna(subset=[\"text\"], inplace = True)\n",
    "# Convert to datetime for the splits\n",
    "data['date'] =  pd.to_datetime(data['date'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Split by events (custom)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To create splits of the data for different time-periods it is sufficient to only run the cells below.\n",
    "The upper part of the notebook only needs to run once to create the processed_data.csv file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "'''\n",
    "# List of notable events\n",
    "griner_release = pd.Timestamp('2022-10-07')\n",
    "musk_twitter_takeover = pd.Timestamp('2022-10-01')\n",
    "pelosi_attacked = pd.Timestamp(\"2022-10-26\")\n",
    "colorado_springs_shooting = pd.Timestamp(\"2022-11-18\")\n",
    "\n",
    "# did not work\n",
    "word_cup = pd.Timestamp('2022-11-01')\n",
    "seoul_halloween = pd.Timestamp('2022-10-28')''';"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "# PARAMS TO MODIFY MANUALLY\n",
    "'''\n",
    "# sub_dir = \"colorado_springs\"\n",
    "splits = [colorado_springs_shooting]\n",
    "months = [pd.Timestamp(\"2022-11-18\")]\n",
    "\n",
    "# save_to_csv(data, splits, sub_dir) currently not used''';"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Split in quarters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sub_dir = \"quarter\"\n",
    "quarterly_dates = pd.date_range(start='2022-06-01', end='2023-04-28', freq='QS-JUN')\n",
    "print(quarterly_dates)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_to_csv(data, quarterly_dates, sub_dir)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Split by months"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "sub_dir = \"monthly\"\n",
    "# Create a list of first-of-the-month timestamps\n",
    "first_of_month_dates = pd.date_range(start='2022-06-01', end='2023-04-28', freq='MS')\n",
    "print(first_of_month_dates)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "save_to_csv(data, first_of_month_dates, sub_dir)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "formatted_range"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
