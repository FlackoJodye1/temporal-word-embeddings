{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data-Analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import nltk"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Read data from processed csv files"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "DIR = Path(\"data\")\n",
    "split_dir = DIR / \"split\"\n",
    "sub_dir = \"quarter\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "if sub_dir:\n",
    "    csv_files = glob.glob(str(split_dir / sub_dir / \"*.csv\"))\n",
    "else:\n",
    "    csv_files = glob.glob(str(split_dir / \"*/*.csv\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "dataframes = []\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "    dataframes.append(df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# used for training the Cade-Compass\n",
    "df_all = pd.concat(dataframes, ignore_index=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported 4 files\n",
      "Data from Period 1: 2022-06-01 23:00:00 - 2022-08-31 23:54:58\n",
      "Data from Period 2: 2022-09-01 00:05:10 - 2022-11-30 23:58:00\n",
      "Data from Period 3: 2022-12-01 00:00:00 - 2023-02-28 23:59:44\n",
      "Data from Period 4: 2023-03-01 00:00:00 - 2023-04-28 07:44:34\n"
     ]
    }
   ],
   "source": [
    "print(f\"Imported {len(dataframes)} files\")\n",
    "\n",
    "for i, df in enumerate(dataframes):\n",
    "    print(f\"Data from Period {i+1}: {df.date.min()} - {df.date.max()}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "# convert date back to datetime object and sort them by date\n",
    "for df in dataframes:\n",
    "    df['date'] =  pd.to_datetime(df['date'])\n",
    "    df.sort_values('date', inplace=True)\n",
    "\n",
    "df_all['date'] =  pd.to_datetime(df_all['date'])\n",
    "df_all.sort_values('date', inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check if any missing values are in the (processed) text column"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 10,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all[\"text\"].isna().sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If there are any but still amount to only an insignificant portion of the data, we delete them as to not cause problems with nltk's tokenizers."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "df_all.dropna(subset=['text'], inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sample-Engine"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "***CBS-News - The Hottest Topic of each Month:***\n",
    "\n",
    "***2022***\n",
    "* Roe / Wade / Abortion (June)\n",
    "* Shinzo / Abe / Japan(July)\n",
    "* Trump / Mar-a-Lago (August)\n",
    "* Queen / Elizabeth / England (September)\n",
    "* Elon / Musk / Twitter (October)\n",
    "* Republicans / Red / Wave (November)\n",
    "* Russia / Brittney / Griner / Prisoner (December)\n",
    "\n",
    "***2023 (TODO)***\n",
    "* xxx (January)\n",
    "* xxx (February)\n",
    "* xxx (March)\n",
    "* xxx (April)\n",
    "\n",
    "Timeframe: 2022-06-01 to 2023-04-28\n",
    "source: [cbsnews](https://www.cbsnews.com/news/the-year-in-review-top-news-stories-of-2022-month-by-month/)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "def sample_text(df, month = \"06\"):\n",
    "    sample_stream_1 = df.sample(1)\n",
    "    print(\"Stream 1\")\n",
    "    print(f\"Date: {sample_stream_1['date'].iloc[0].date()}\")\n",
    "    print(f\"Sentiment: {sample_stream_1['sentiment'].iloc[0]}\")\n",
    "    print(\"----------------\")\n",
    "    print(sample_stream_1[\"text\"].iloc[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Notable Events:\n",
    "\n",
    "* Teacherstrike\n",
    "* libsoftiktok\n",
    "* Paul Vallas\n",
    "* Transgender"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream 1\n",
      "Date: 2022-06-14\n",
      "Sentiment: Neutrals\n",
      "----------------\n",
      " joanntrejo impacting faculty diversity  sdiracda thrilled to announce 6 ucsandiego iracda fellows obtain tenuretrack faculty positions the new stewards of diversity inclusivity trainers of next gen stem students across the us nigmstraining nindsdiversity nihcoswd nihnhlbi pictwittercomsmwiutnz1v\n"
     ]
    }
   ],
   "source": [
    "sample_text(df_all)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create Corpora"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [],
   "source": [
    "corpus_all = df_all[\"text\"].values.tolist()\n",
    "\n",
    "corpora = [df[\"text\"].values.tolist() for df in dataframes]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create Tokens"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/149998 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "d9687e91f15f4550a7cfbf358d097a30"
      },
      "application/json": {
       "n": 0,
       "total": 149998,
       "elapsed": 0.004676103591918945,
       "ncols": null,
       "nrows": null,
       "prefix": "",
       "ascii": false,
       "unit": "it",
       "unit_scale": false,
       "rate": null,
       "bar_format": null,
       "postfix": null,
       "unit_divisor": 1000,
       "initial": 0,
       "colour": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "41559258760a4cb5871d6cfef32d9469"
      },
      "application/json": {
       "n": 0,
       "total": 4,
       "elapsed": 0.0020220279693603516,
       "ncols": null,
       "nrows": null,
       "prefix": "",
       "ascii": false,
       "unit": "it",
       "unit_scale": false,
       "rate": null,
       "bar_format": null,
       "postfix": null,
       "unit_divisor": 1000,
       "initial": 0,
       "colour": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "\n",
    "tokens_all = tokenizer.tokenize(\" \".join(str(text) for text in tqdm(corpus_all)))\n",
    "\n",
    "tokens = [tokenizer.tokenize(\" \".join(str(text) for text in corpus)) for corpus in tqdm(corpora)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create Vocabularies"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/4 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "023eab7543974b06a0e34ef14064652d"
      },
      "application/json": {
       "n": 0,
       "total": 4,
       "elapsed": 0.12035012245178223,
       "ncols": null,
       "nrows": null,
       "prefix": "",
       "ascii": false,
       "unit": "it",
       "unit_scale": false,
       "rate": null,
       "bar_format": null,
       "postfix": null,
       "unit_divisor": 1000,
       "initial": 0,
       "colour": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# unique vocabulary of each class\n",
    "vocabulary_all = set(tokens_all)\n",
    "\n",
    "vocabularies = [set(tokens_split) for tokens_split in tqdm(tokens)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "vocabulary_size_all = len(vocabulary_all)\n",
    "\n",
    "vocabulary_sizes = [len(vocabulary) for vocabulary in vocabularies]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "data": {
      "text/plain": "386"
     },
     "execution_count": 18,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[1].count(\"shooting\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 220582\n",
      "Vocabulary size of split 1: 90027\n",
      "Vocabulary size of split 2: 100378\n",
      "Vocabulary size of split 3: 94680\n",
      "Vocabulary size of split 4: 67276\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocabulary size: {vocabulary_size_all}\")\n",
    "for i, vocab_size in enumerate(vocabulary_sizes):\n",
    "    print(f\"Vocabulary size of split {i+1}: {vocab_size}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Word Embeddings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. Static: Word2Vec (Gensim)\n",
    "\n",
    "2. Temporal: TWEC/CADE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Word2Vec (static)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/149998 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "4e2dc7943daf41e7a9c278f823a51ebf"
      },
      "application/json": {
       "n": 0,
       "total": 149998,
       "elapsed": 0.004467964172363281,
       "ncols": null,
       "nrows": null,
       "prefix": "",
       "ascii": false,
       "unit": "it",
       "unit_scale": false,
       "rate": null,
       "bar_format": null,
       "postfix": null,
       "unit_divisor": 1000,
       "initial": 0,
       "colour": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "tokenized_sentences_all = [word_tokenize(item) for item in tqdm(corpus_all)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(tokenized_sentences_all, min_count = 5, seed=1040)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "data": {
      "text/plain": "[('biden', 0.7552129626274109),\n ('donald', 0.7537271976470947),\n ('trumps', 0.7485966682434082),\n ('obama', 0.7285426259040833),\n ('thenpresident', 0.6505183577537537),\n ('joe', 0.6470562219619751),\n ('presidency', 0.6236420273780823),\n ('desantis', 0.6080209612846375),\n ('barack', 0.5927667617797852),\n ('bidens', 0.5920045971870422)]"
     },
     "execution_count": 23,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_word = \"trump\"\n",
    "w2v_model.wv.most_similar(test_word)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "w2v_model.save(\"model/static/word2vec.model\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Temporal: TWEC/CADE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "***Temporal Word Embeddings with a Compass***\n",
    "\n",
    "* [Source-Code](https://github.com/valedica/twec)\n",
    "\n",
    "* [Paper](https://arxiv.org/abs/1906.02376)\n",
    "\n",
    "* [Blogpost](https://fede-bianchi.medium.com/aligning-temporal-diachronic-word-embeddings-with-a-compass-732ab7427955)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Save the concatenated text to txt-files to make them usable for Cade"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "1.10.0\n"
     ]
    }
   ],
   "source": [
    "import smart_open\n",
    "print(smart_open.__version__)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [],
   "source": [
    "from cade.cade import CADE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "cade_dir = DIR / \"cade\"\n",
    "cade_split_dir = cade_dir / sub_dir\n",
    "cade_split_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "file_paths = [(cade_split_dir / csv_file.split(\"/\")[-1].split(\".\")[0]).with_suffix(\".txt\") for csv_file in csv_files]\n",
    "\n",
    "file_paths_and_corpora = {\n",
    "    cade_dir / 'compass.txt': corpus_all\n",
    "}\n",
    "\n",
    "for key, value in zip(file_paths, corpora):\n",
    "    file_paths_and_corpora[key] = value\n",
    "\n",
    "for file_path, corpus in file_paths_and_corpora.items():\n",
    "    with open(file_path, 'w') as file:\n",
    "        for item in corpus:\n",
    "            file.write(\"%s\\n\" % item)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "# only needed once for the installation & creation of a venv\n",
    "'''%%capture\n",
    "!pip install -U cade\n",
    "!pip install git+https://github.com/vinid/gensim.git''';"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create & train the compass\n",
    "\n",
    "This creates atemporal context and target word embeddings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "3.8.0\n"
     ]
    }
   ],
   "source": [
    "import gensim\n",
    "print(gensim.__version__)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the compass from scratch.\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "aligner = CADE(size=30, min_count = 5)\n",
    "aligner.train_compass(str((cade_dir / \"compass\").with_suffix(\".txt\")), overwrite=True);"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [
    {
     "data": {
      "text/plain": "[PosixPath('data/cade/quarter/01_Jun_to_01_Sep.txt'),\n PosixPath('data/cade/quarter/01_Sep_to_01_Dec.txt'),\n PosixPath('data/cade/quarter/01_Dec_to_01_Mar.txt'),\n PosixPath('data/cade/quarter/01_Mar_to_28_Apr.txt')]"
     },
     "execution_count": 31,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_paths"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training embeddings: slice data/cade/quarter/01_Jun_to_01_Sep.txt.\n",
      "Initializing embeddings from compass.\n",
      "Training embeddings: slice data/cade/quarter/01_Sep_to_01_Dec.txt.\n",
      "Initializing embeddings from compass.\n",
      "Training embeddings: slice data/cade/quarter/01_Dec_to_01_Mar.txt.\n",
      "Initializing embeddings from compass.\n",
      "Training embeddings: slice data/cade/quarter/01_Mar_to_28_Apr.txt.\n",
      "Initializing embeddings from compass.\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# train slices, they will be already aligned\n",
    "slices = [aligner.train_slice(file_path, save=True) for file_path in file_paths] # list of gensim word2vec objects"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load the models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"done\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
