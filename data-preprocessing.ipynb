{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data Preprocessing"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import pandas as pd\n",
    "from pathlib import Path\n",
    "import warnings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# Set random seed\n",
    "np.random.seed(1040)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "DIR = Path(\"data\")\n",
    "input_dir = DIR / \"input\"\n",
    "stream_1_path = input_dir / \"Stream1.xlsx\"\n",
    "stream_2_path = input_dir / \"Stream2.xlsx\"\n",
    "stream_3_path = input_dir / \"Stream3.xlsx\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [],
   "source": [
    "# this is ok and will not cause problems\n",
    "warnings.filterwarnings(\"ignore\", message=\"Workbook contains no default style, apply openpyxl's default\")\n",
    "\n",
    "stream_1_data = pd.read_excel(stream_2_path, engine=\"openpyxl\") # stream 1 contains the chronologically second part\n",
    "stream_2_data = pd.read_excel(stream_1_path, engine=\"openpyxl\") # stream 2 contains the chronologically first part\n",
    "stream_3_data = pd.read_excel(stream_3_path, engine=\"openpyxl\")\n",
    "\n",
    "data = pd.concat([stream_1_data, stream_2_data, stream_3_data], ignore_index=True);"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## First Look"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The data contains a great number of different attributes for each observation.\n",
    "therefore we start by looking at the attributes to get an idea of what to keep and what to get rid of."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "First we convert all attribute-names to lowercase and replace white-spaces to underscores to make things simpler"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "data.columns = [c.replace(' ', '_').lower() for c in stream_1_data.columns]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'pandas.core.frame.DataFrame'>\n",
      "RangeIndex: 150024 entries, 0 to 150023\n",
      "Data columns (total 63 columns):\n",
      " #   Column                                       Non-Null Count   Dtype  \n",
      "---  ------                                       --------------   -----  \n",
      " 0   post_id                                      143071 non-null  object \n",
      " 1   sound_bite_text                              150015 non-null  object \n",
      " 2   ratings_and_scores                           0 non-null       float64\n",
      " 3   title                                        71124 non-null   object \n",
      " 4   source_type                                  143053 non-null  object \n",
      " 5   post_type                                    85476 non-null   object \n",
      " 6   is_paid                                      150000 non-null  object \n",
      " 7   media_type                                   143053 non-null  object \n",
      " 8   url                                          143053 non-null  object \n",
      " 9   media_link                                   17428 non-null   object \n",
      " 10  domain                                       143053 non-null  object \n",
      " 11  sentiment                                    143053 non-null  object \n",
      " 12  published_date_(gmt+01:00)_london            150000 non-null  object \n",
      " 13  author_gender                                143053 non-null  object \n",
      " 14  author_url                                   60474 non-null   object \n",
      " 15  author_name                                  95676 non-null   object \n",
      " 16  author_handle                                41075 non-null   object \n",
      " 17  author_id                                    42424 non-null   object \n",
      " 18  author_location_-_country_1                  93711 non-null   object \n",
      " 19  author_location_-_state/province_1           14407 non-null   object \n",
      " 20  author_location_-_city_1                     10102 non-null   object \n",
      " 21  author_location_-_country_2                  126 non-null     object \n",
      " 22  author_location_-_state/province_2           114 non-null     object \n",
      " 23  author_location_-_city_2                     90 non-null      object \n",
      " 24  author_location_-_other                      13 non-null      object \n",
      " 25  author_reddit_karma                          143053 non-null  object \n",
      " 26  followers/daily_unique_visitors/subscribers  52386 non-null   float64\n",
      " 27  professions                                  4349 non-null    object \n",
      " 28  interests                                    8941 non-null    object \n",
      " 29  positive_objects                             27434 non-null   object \n",
      " 30  negative_objects                             32045 non-null   object \n",
      " 31  richness                                     143053 non-null  float64\n",
      " 32  tags                                         0 non-null       float64\n",
      " 33  quoted_post                                  558 non-null     object \n",
      " 34  quoted_author_name                           558 non-null     object \n",
      " 35  quoted_author_handle                         558 non-null     object \n",
      " 36  total_engagements                            15897 non-null   float64\n",
      " 37  post_comments                                5549 non-null    float64\n",
      " 38  post_likes                                   14281 non-null   float64\n",
      " 39  post_shares                                  1017 non-null    float64\n",
      " 40  post_views                                   0 non-null       float64\n",
      " 41  post_dislikes                                0 non-null       float64\n",
      " 42  reddit_score                                 143053 non-null  object \n",
      " 43  product_name                                 0 non-null       float64\n",
      " 44  product_hierarchy                            0 non-null       float64\n",
      " 45  rating                                       65 non-null      float64\n",
      " 46  @mention_media_tags                          0 non-null       float64\n",
      " 47  source_name                                  13715 non-null   object \n",
      " 48  lexisnexis_source_publisher                  0 non-null       float64\n",
      " 49  lexisnexis_source_category                   0 non-null       float64\n",
      " 50  lexisnexis_source_genre                      0 non-null       float64\n",
      " 51  lexisnexis_source_quality                    0 non-null       float64\n",
      " 52  lexisnexis_company_-_high                    0 non-null       float64\n",
      " 53  lexisnexis_company_-_any                     0 non-null       float64\n",
      " 54  lexisnexis_person_-_high                     0 non-null       float64\n",
      " 55  lexisnexis_person_-_any                      0 non-null       float64\n",
      " 56  lexisnexis_institution_-_high                0 non-null       float64\n",
      " 57  lexisnexis_institution_-_any                 0 non-null       float64\n",
      " 58  lexisnexis_subject_group_1                   0 non-null       float64\n",
      " 59  lexisnexis_subject_1                         0 non-null       float64\n",
      " 60  lexisnexis_subject_group_2                   0 non-null       float64\n",
      " 61  lexisnexis_subject_2                         0 non-null       float64\n",
      " 62  lexisnexis_other_subjects                    0 non-null       float64\n",
      "dtypes: float64(29), object(34)\n",
      "memory usage: 72.1+ MB\n"
     ]
    }
   ],
   "source": [
    "data.info()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Important:\n",
    "\n",
    "- post_id (ID)\n",
    "- Sound Bite Text (main text corpus)\n",
    "- Published Date (GMT+01:00) London (used to create dynamic embeddings)\n",
    "- Sentiment (used for extrinsic evaluation)\n",
    "\n",
    "In the following I focus first on these attributes to keep things clear and simple"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "important_attributes = [\"post_id\", \"sound_bite_text\", \"published_date_(gmt+01:00)_london\", \"sentiment\"]\n",
    "\n",
    "data = pd.DataFrame(data, columns=important_attributes)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "For simplicity's sake I chose to rename the attributes to a more readable and manageable form"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "data.rename(columns={\"source_type\": \"source\", \"sound_bite_text\":\"raw_text\", \"published_date_(gmt+01:00)_london\": \"date\", \"post_id\":\"id\"}, inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Lets look at how many values are actually there for the selected attributes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "data": {
      "text/plain": "sentiment    0.046466\nid           0.046346\ndate         0.000160\nraw_text     0.000060\ndtype: float64"
     },
     "execution_count": 11,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# number of observations\n",
    "n = len(data)\n",
    "\n",
    "# Display relative counts of missing values\n",
    "data.isnull().sum().divide(n).sort_values(ascending=False)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Both the date and the text have almost no missing values, which is the main thing.\n",
    "The attribute sentiment will only be used for a part of the evaluation of the embeddings and is therefore not as important.\n",
    "I therefore decide to go for the following strategy:\n",
    "\n",
    "Remove observations:\n",
    "\n",
    "- with missing date\n",
    "- with missing text\n",
    "\n",
    "Keep observations:\n",
    "- with missing sentiment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Let us now describe the key characteristics of our (remaining) data"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "                                                 id  \\\n0   101043269988443_685479222937141_641761047285352   \n1  102479025168087_499693558822805_1195202924611670   \n2     10643211755_10161745802786756_627308188838071   \n3    10643211755_10161748775911756_1235289853921186   \n4  101043269988443_687503842734679_3330045383891840   \n\n                                            raw_text                     date  \\\n0  Check this guy out at a school board meeting. ...  Sep 16, 2022 6:29:09 AM   \n1  Trump's one race theory like that of Hitler is...  Jul 31, 2022 1:10:33 AM   \n2  Jon C Treleaven Seems right! I believe in pare...  Sep 26, 2022 5:17:42 PM   \n3  This happening in my town. We’re having school...  Sep 27, 2022 4:23:12 AM   \n4  The blame for all these perverted lifestyles b...   Sep 5, 2022 6:10:30 PM   \n\n   sentiment  \n0   Neutrals  \n1   Neutrals  \n2   Neutrals  \n3  Positives  \n4   Neutrals  ",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>id</th>\n      <th>raw_text</th>\n      <th>date</th>\n      <th>sentiment</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>0</th>\n      <td>101043269988443_685479222937141_641761047285352</td>\n      <td>Check this guy out at a school board meeting. ...</td>\n      <td>Sep 16, 2022 6:29:09 AM</td>\n      <td>Neutrals</td>\n    </tr>\n    <tr>\n      <th>1</th>\n      <td>102479025168087_499693558822805_1195202924611670</td>\n      <td>Trump's one race theory like that of Hitler is...</td>\n      <td>Jul 31, 2022 1:10:33 AM</td>\n      <td>Neutrals</td>\n    </tr>\n    <tr>\n      <th>2</th>\n      <td>10643211755_10161745802786756_627308188838071</td>\n      <td>Jon C Treleaven Seems right! I believe in pare...</td>\n      <td>Sep 26, 2022 5:17:42 PM</td>\n      <td>Neutrals</td>\n    </tr>\n    <tr>\n      <th>3</th>\n      <td>10643211755_10161748775911756_1235289853921186</td>\n      <td>This happening in my town. We’re having school...</td>\n      <td>Sep 27, 2022 4:23:12 AM</td>\n      <td>Positives</td>\n    </tr>\n    <tr>\n      <th>4</th>\n      <td>101043269988443_687503842734679_3330045383891840</td>\n      <td>The blame for all these perverted lifestyles b...</td>\n      <td>Sep 5, 2022 6:10:30 PM</td>\n      <td>Neutrals</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "(150024, 4)"
     },
     "execution_count": 14,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.shape"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [
    {
     "data": {
      "text/plain": "id           object\nraw_text     object\ndate         object\nsentiment    object\ndtype: object"
     },
     "execution_count": 15,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data.dtypes"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The datatypes are mostly as we would like.\n",
    "We only convert the date attribute from object to date, since we are working with a time series."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "24"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data['date'].isnull().sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Given that temporal word embeddings heavily rely on dates, we consider the date to be crucial. However, out of the 24 tweets available, some lack a date, so I opt to eliminate those observations from the dataset."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [],
   "source": [
    "data.dropna(subset=['date'], inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "data['date'] =  pd.to_datetime(data['date'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since I focus on the temporal change of words, I chose to sort the observations by date because that makes a manual inspection later on more convenient"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "data.sort_values('date', inplace=True);"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "The text of the tweets is the main source of information, lets look how many missing values we encounter here"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Obsersations with no text: 0\n",
      "Obsersations with empty text: 0\n"
     ]
    }
   ],
   "source": [
    "null_texts = data['raw_text'].isnull().sum()\n",
    "empty_texts = data[data['raw_text'].str.len() < 2].count().iloc[0]\n",
    "print(f\"Obsersations with no text: {null_texts}\")\n",
    "print(f\"Obsersations with empty text: {empty_texts}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Since it is only one observation we can safely remove it to prevent it from causing errors later on."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "data.dropna(subset=['raw_text'], inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now lets take a look at the different attributes. Since the task at hand is a sentiment analysis, we focus on this attribute first"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "array(['Positives', 'Neutrals', nan, 'Negatives', 'Mixed'], dtype=object)"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"sentiment\"].unique()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "So our target is to predict the sentiment from the text (sound_bite_text).\n",
    "The sentiment is either:\n",
    "\n",
    "- Positive\n",
    "- Negative\n",
    "- Neutral\n",
    "- Mixed"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Period of time: ('2022-06-01', '2023-04-28')\n"
     ]
    }
   ],
   "source": [
    "# Get the range of dates\n",
    "period = (data['date'].min(), data['date'].max())\n",
    "\n",
    "# Format the output\n",
    "formatted_range = tuple(date.strftime(\"%Y-%m-%d\") for date in period)\n",
    "print(\"Period of time:\", formatted_range)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 1. Convert to lowercase"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [],
   "source": [
    "data[\"text\"] = data[\"raw_text\"].str.lower()\n",
    "\n",
    "# rearrange columns\n",
    "data = data[['id', 'text', \"raw_text\", 'date', 'sentiment']]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 2. Remove Unicode Characters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Eliminate the punctuation, URL, and @"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 56,
   "outputs": [],
   "source": [
    "import re\n",
    "\n",
    "def clean_text(text):\n",
    "\n",
    "    # Removes all of them\n",
    "    text = re.sub(r\"(@\\[A-Za-z0-9]+)|([^0-9A-Za-z \\t])|(\\w+:\\/\\/\\S+)|^rt|http.+?\", \"\", text)\n",
    "\n",
    "    return text"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 57,
   "outputs": [],
   "source": [
    "data[\"text\"] = data[\"text\"].apply(clean_text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 3. Remove Stopwords"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import nltk\n",
    "nltk.download('punkt')\n",
    "\n",
    "def remove_stopwords(text):\n",
    "    from nltk.corpus import stopwords\n",
    "    from nltk.tokenize import word_tokenize\n",
    "\n",
    "    stop_words = set(stopwords.words(\"english\"))\n",
    "    word_tokens = word_tokenize(text)\n",
    "    filtered_text = [word for word in word_tokens if word not in stop_words]\n",
    "    return \" \".join(filtered_text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 59,
   "outputs": [],
   "source": [
    "# data[\"text\"] = data[\"text\"].apply(remove_stopwords)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Note: No Stopword removal is done at the moment**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### 4. Stemming (Optional)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Good for extrinsic evaluation but bad for visualization"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 60,
   "outputs": [],
   "source": [
    "from nltk.stem import SnowballStemmer\n",
    "from nltk.tokenize import word_tokenize\n",
    "\n",
    "def perform_stemming(text):\n",
    "    stemmer = SnowballStemmer(language = \"english\")\n",
    "    word_tokens = word_tokenize(text)\n",
    "    stemmed_text = [stemmer.stem(word) for word in word_tokens]\n",
    "    return \" \".join(stemmed_text)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "**Note: No Stemming is done at the moment**"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 61,
   "outputs": [],
   "source": [
    "# data[\"text\"] = data[\"text\"].apply(perform_stemming)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check if we inadvertently created some Na, Null Values in our (processed) text column"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 62,
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 62,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"text\"].isna().sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 63,
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 63,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "data[\"text\"].isnull().sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "def save_to_csv(data, splits: list, sub_dir: str):\n",
    "\n",
    "    output_dir = Path(\"data/split/\") / sub_dir\n",
    "    # Create output directory if it doesn't exist\n",
    "    output_dir.mkdir(parents=True, exist_ok=True)\n",
    "    # range of the observations\n",
    "    lower = data[\"date\"].min()\n",
    "    upper = data[\"date\"].max()\n",
    "\n",
    "    for split in splits:\n",
    "        split_df = data[(lower <= data['date']) & (data['date'] < split)]\n",
    "        split_filename = output_dir / f\"{lower.strftime('%d_%b')}_to_{split.strftime('%d_%b')}.csv\"\n",
    "        print(f\"{lower.strftime('%d_%b')}_to_{split.strftime('%d_%b')}.csv\")\n",
    "        # Save the filtered data to csv, overwrite if exists\n",
    "        split_df.to_csv(split_filename, index=False, mode='w')\n",
    "        # Update the lower date for the next iteration\n",
    "        lower = split\n",
    "\n",
    "    # take care of second half of the last split\n",
    "    split_df = data[(lower <= data['date']) & (data['date'] <= upper)]\n",
    "    split_filename = output_dir / f\"{lower.strftime('%d_%b')}_to_{upper.strftime('%d_%b')}.csv\"\n",
    "    split_df.to_csv(split_filename, index=False, mode='w')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 65,
   "outputs": [],
   "source": [
    "cleaned_data_path = DIR / \"processed_data.csv\"\n",
    "data.to_csv(cleaned_data_path, index=False, mode='w')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Split"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "cleaned_data_path = DIR / \"processed_data.csv\"\n",
    "data = pd.read_csv(cleaned_data_path)\n",
    "\n",
    "# Remove text null texts which occurred due to preprocessing and saving to csv\n",
    "data.dropna(subset=[\"text\"], inplace = True)\n",
    "# Convert to datetime for the splits\n",
    "data['date'] =  pd.to_datetime(data['date'])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Split by events (custom)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To create splits of the data for different time-periods it is sufficient to only run the cells below.\n",
    "The upper part of the notebook only needs to run once to create the processed_data.csv file"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "# List of notable events\n",
    "griner_release = pd.Timestamp('2022-10-07')\n",
    "musk_twitter_takeover = pd.Timestamp('2022-10-01')\n",
    "pelosi_attacked = pd.Timestamp(\"2022-10-26\")\n",
    "colorado_springs_shooting = pd.Timestamp(\"2022-11-18\")\n",
    "\n",
    "# did not work\n",
    "word_cup = pd.Timestamp('2022-11-01')\n",
    "seoul_halloween = pd.Timestamp('2022-10-28')"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 72,
   "outputs": [],
   "source": [
    "# PARAMS TO MODIFY MANUALLY\n",
    "sub_dir = \"colorado_springs\"\n",
    "splits = [colorado_springs_shooting]\n",
    "\n",
    "months = [pd.Timestamp(\"2022-11-18\")]\n",
    "\n",
    "save_to_csv(data, splits, sub_dir)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Split in quarters"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2022-06-01', '2022-09-01', '2022-12-01', '2023-03-01'], dtype='datetime64[ns]', freq='QS-JUN')\n"
     ]
    }
   ],
   "source": [
    "sub_dir = \"quarter\"\n",
    "quarterly_dates = pd.date_range(start='2022-06-01', end='2023-04-28', freq='QS-JUN')\n",
    "print(quarterly_dates)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "lower\n",
      "2022-06-01 23:00:00\n",
      "upper\n",
      "2022-06-01 00:00:00\n",
      "filename\n",
      "01_Jun_to_01_Jun.csv\n",
      "lower\n",
      "2022-06-01 00:00:00\n",
      "upper\n",
      "2022-09-01 00:00:00\n",
      "filename\n",
      "01_Jun_to_01_Sep.csv\n",
      "lower\n",
      "2022-09-01 00:00:00\n",
      "upper\n",
      "2022-12-01 00:00:00\n",
      "filename\n",
      "01_Sep_to_01_Dec.csv\n",
      "lower\n",
      "2022-12-01 00:00:00\n",
      "upper\n",
      "2023-03-01 00:00:00\n",
      "filename\n",
      "01_Dec_to_01_Mar.csv\n"
     ]
    }
   ],
   "source": [
    "save_to_csv(data, quarterly_dates, sub_dir)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Split by months"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "DatetimeIndex(['2022-06-01', '2022-07-01', '2022-08-01', '2022-09-01',\n",
      "               '2022-10-01', '2022-11-01', '2022-12-01', '2023-01-01',\n",
      "               '2023-02-01', '2023-03-01', '2023-04-01'],\n",
      "              dtype='datetime64[ns]', freq='MS')\n"
     ]
    }
   ],
   "source": [
    "sub_dir = \"monthly\"\n",
    "# Create a list of first-of-the-month timestamps\n",
    "first_of_month_dates = pd.date_range(start='2022-06-01', end='2023-04-28', freq='MS')\n",
    "print(first_of_month_dates)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [],
   "source": [
    "save_to_csv(data, first_of_month_dates, sub_dir)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "('2022-06-01', '2023-04-28')"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "formatted_range"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
