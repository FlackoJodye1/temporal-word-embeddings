{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data-Analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nltk"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Read data from processed csv files"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [],
   "source": [
    "current_split_dir = \"colorado_springs\"\n",
    "\n",
    "DIR = Path(\"data/\")\n",
    "input_dir = DIR / \"split\" / current_split_dir\n",
    "csv_files = glob.glob(str(input_dir / \"*.csv\"))\n",
    "\n",
    "dataframes = []\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "    dataframes.append(df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "outputs": [],
   "source": [
    "df_all = pd.concat(dataframes, ignore_index=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported 2 files\n",
      "Data from Period 1: 2022-11-18 00:00:00 - 2023-04-28 07:44:34\n",
      "Data from Period 2: 2022-06-01 23:00:00 - 2022-11-17 23:59:17\n"
     ]
    }
   ],
   "source": [
    "print(f\"Imported {len(dataframes)} files\")\n",
    "\n",
    "counter = 1\n",
    "for df in dataframes:\n",
    "    print(f\"Data from Period {counter}: {df.date.min()} - {df.date.max()}\")\n",
    "    counter += 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "outputs": [],
   "source": [
    "# convert date back to datetime object and sort them by date\n",
    "for df in dataframes:\n",
    "    df['date'] =  pd.to_datetime(df['date'])\n",
    "    df.sort_values('date', inplace=True)\n",
    "\n",
    "df_all['date'] =  pd.to_datetime(df_all['date'])\n",
    "df_all.sort_values('date', inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check if any missing values are in the (processed) text column"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "outputs": [
    {
     "data": {
      "text/plain": "109"
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all[\"text\"].isna().sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If there are any but still amount to only an insignificant portion of the data, we delete them as to not cause problems with nltk's tokenizers."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "outputs": [],
   "source": [
    "df_all.dropna(subset=['text'], inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sample-Engine"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "***CBS-News - The Hottest Topic of each Month:***\n",
    "\n",
    "***2022***\n",
    "* Roe / Wade / Abortion (June)\n",
    "* Shinzo / Abe / Japan(July)\n",
    "* Trump / Mar-a-Lago (August)\n",
    "* Queen / Elizabeth / England (September)\n",
    "* Elon / Musk / Twitter (October)\n",
    "* Republicans / Red / Wave (November)\n",
    "* Russia / Brittney / Griner / Prisoner (December)\n",
    "\n",
    "***2023 (TODO)***\n",
    "* xxx (January)\n",
    "* xxx (February)\n",
    "* xxx (March)\n",
    "* xxx (April)\n",
    "\n",
    "Timeframe: 2022-06-01 to 2023-04-28\n",
    "source: [cbsnews](https://www.cbsnews.com/news/the-year-in-review-top-news-stories-of-2022-month-by-month/)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create Corpora"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "outputs": [],
   "source": [
    "corpus_all = df_all[\"text\"].values.tolist()\n",
    "\n",
    "corpora = [df[\"text\"].values.tolist() for df in dataframes]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create Tokens"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149891/149891 [00:00<00:00, 4847736.63it/s]\n",
      "100%|██████████| 2/2 [00:08<00:00,  4.04s/it]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "\n",
    "tokens_all = tokenizer.tokenize(\" \".join(str(text) for text in tqdm(corpus_all)))\n",
    "\n",
    "tokens = [tokenizer.tokenize(\" \".join(str(text) for text in corpus)) for corpus in tqdm(corpora)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create Vocabularies"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 36,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 2/2 [00:00<00:00,  7.75it/s]\n"
     ]
    }
   ],
   "source": [
    "# unique vocabulary of each class\n",
    "vocabulary_all = set(tokens_all)\n",
    "\n",
    "vocabularies = [set(tokens_split) for tokens_split in tqdm(tokens)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "outputs": [],
   "source": [
    "vocabulary_size_all = len(vocabulary_all)\n",
    "\n",
    "vocabulary_sizes = [len(vocabulary) for vocabulary in vocabularies]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 38,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 187589\n",
      "Vocabulary size of split 1: 110841\n",
      "Vocabulary size of split 2: 118035\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocabulary size: {vocabulary_size_all}\")\n",
    "counter = 1\n",
    "for vocab_size in vocabulary_sizes:\n",
    "    print(f\"Vocabulary size of split {counter}: {vocab_size}\")\n",
    "    counter += 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Word Embeddings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. Static: Word2Vec (Gensim)\n",
    "\n",
    "2. Temporal: TWEC/CADE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Word2Vec (static)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 39,
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 40,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 149891/149891 [00:13<00:00, 10883.08it/s]\n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "tokenized_sentences_all = [word_tokenize(item) for item in tqdm(corpus_all)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 41,
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(tokenized_sentences_all, min_count = 1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 42,
   "outputs": [
    {
     "data": {
      "text/plain": "[('biden', 0.742926299571991),\n ('donald', 0.7195136547088623),\n ('obama', 0.6923248171806335),\n ('egogener', 0.6524394750595093),\n ('impeach', 0.6318155527114868),\n ('presidenti', 0.6065165996551514),\n ('thenpresid', 0.5993049144744873),\n ('pompeo', 0.591389000415802),\n ('gop', 0.5850652456283569),\n ('crist', 0.5757563710212708)]"
     },
     "execution_count": 42,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_word = \"trump\"\n",
    "w2v_model.wv.most_similar(test_word)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 43,
   "outputs": [],
   "source": [
    "w2v_model.save(\"model/word2vec.model\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Temporal: TWEC/CADE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "***Temporal Word Embeddings with a Compass***\n",
    "\n",
    "* [Source-Code](https://github.com/valedica/twec)\n",
    "\n",
    "* [Paper](https://arxiv.org/abs/1906.02376)\n",
    "\n",
    "* [Blogpost](https://fede-bianchi.medium.com/aligning-temporal-diachronic-word-embeddings-with-a-compass-732ab7427955)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Save the concatenated text to txt-files to make them usable for Cade"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "outputs": [],
   "source": [
    "from cade.cade import CADE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "outputs": [],
   "source": [
    "cade_dir = DIR / \"cade\"\n",
    "cade_split_dir = cade_dir / current_split_dir\n",
    "cade_split_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "file_paths = [(cade_split_dir / csv_file.split(\"/\")[-1].split(\".\")[0]).with_suffix(\".txt\") for csv_file in csv_files]\n",
    "\n",
    "file_paths_and_corpora = {\n",
    "    cade_dir / 'compass.txt': corpus_all\n",
    "}\n",
    "\n",
    "for key, value in zip(file_paths, corpora):\n",
    "    file_paths_and_corpora[key] = value\n",
    "\n",
    "for file_path, corpus in file_paths_and_corpora.items():\n",
    "    with open(file_path, 'w') as file:\n",
    "        for item in corpus:\n",
    "            file.write(\"%s\\n\" % item)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "outputs": [],
   "source": [
    "# only needed once for the installation & creation of a venv\n",
    "''''%%capture\n",
    "!pip install -U cade\n",
    "!pip install git+https://github.com/vinid/gensim.git''';"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create & train the compass\n",
    "\n",
    "This creates atemporal context and target word embeddings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the compass from scratch.\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "aligner = CADE(size=30)\n",
    "aligner.train_compass(str((cade_dir / \"compass\").with_suffix(\".txt\")), overwrite=True);"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "outputs": [
    {
     "data": {
      "text/plain": "[PosixPath('data/cade/colorado_springs/18_Nov_to_28_Apr.txt'),\n PosixPath('data/cade/colorado_springs/01_Jun_to_18_Nov.txt')]"
     },
     "execution_count": 48,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_paths"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 49,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training embeddings: slice data/cade/colorado_springs/18_Nov_to_28_Apr.txt.\n",
      "Initializing embeddings from compass.\n",
      "Training embeddings: slice data/cade/colorado_springs/01_Jun_to_18_Nov.txt.\n",
      "Initializing embeddings from compass.\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# train slices, they will be already aligned\n",
    "slices = [aligner.train_slice(file_path, save=True) for file_path in file_paths] # list of gensim word2vec objects"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load the models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 50,
   "outputs": [],
   "source": [
    "'''from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "model_dir = Path(\"model/\")\n",
    "model_files = glob.glob(str(model_dir / \"*.model\"))\n",
    "\n",
    "models = [Word2Vec.load(model_file) for model_file in model_files]''';"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "outputs": [],
   "source": [
    "'''\n",
    "model_one = models[0]\n",
    "model_two = models[1]\n",
    "''';"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Data from Period 1: 2022-11-18 00:00:00 - 2023-04-28 07:44:34\n",
      "Data from Period 2: 2022-06-01 23:00:00 - 2022-11-17 23:59:17\n"
     ]
    }
   ],
   "source": [
    "counter = 1\n",
    "for df in dataframes:\n",
    "    print(f\"Period {counter}: {df.date.min()} - {df.date.max()}\")\n",
    "    counter += 1"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
