{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data-Analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from tqdm import tqdm\n",
    "\n",
    "import nltk"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Read data from processed csv files"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "DIR = Path(\"data\")\n",
    "split_dir = DIR / \"split\"\n",
    "csv_files = glob.glob(str(split_dir / \"*/*.csv\"))\n",
    "\n",
    "dataframes = []\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "    dataframes.append(df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "# used for training the Cade-Compass\n",
    "df_all = pd.concat(dataframes, ignore_index=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported 8 files\n",
      "Data from Period 1: 2022-06-01 23:00:00 - 2022-09-30 22:58:00\n",
      "Data from Period 2: 2022-10-01 23:01:00 - 2023-04-28 07:44:34\n",
      "Data from Period 3: 2022-10-07 00:00:00 - 2023-04-28 07:44:34\n",
      "Data from Period 4: 2022-06-01 23:00:00 - 2022-10-06 23:56:45\n",
      "Data from Period 5: 2022-06-01 23:00:00 - 2022-10-25 23:52:11\n",
      "Data from Period 6: 2022-10-26 00:00:00 - 2023-04-28 07:44:34\n",
      "Data from Period 7: 2022-11-18 00:00:00 - 2023-04-28 07:44:34\n",
      "Data from Period 8: 2022-06-01 23:00:00 - 2022-11-17 23:59:17\n"
     ]
    }
   ],
   "source": [
    "print(f\"Imported {len(dataframes)} files\")\n",
    "\n",
    "counter = 1\n",
    "for df in dataframes:\n",
    "    print(f\"Data from Period {counter}: {df.date.min()} - {df.date.max()}\")\n",
    "    counter += 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# convert date back to datetime object and sort them by date\n",
    "for df in dataframes:\n",
    "    df['date'] =  pd.to_datetime(df['date'])\n",
    "    df.sort_values('date', inplace=True)\n",
    "\n",
    "df_all['date'] =  pd.to_datetime(df_all['date'])\n",
    "df_all.sort_values('date', inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check if any missing values are in the (processed) text column"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all[\"text\"].isna().sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If there are any but still amount to only an insignificant portion of the data, we delete them as to not cause problems with nltk's tokenizers."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "df_all.dropna(subset=['text'], inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sample-Engine"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "***CBS-News - The Hottest Topic of each Month:***\n",
    "\n",
    "***2022***\n",
    "* Roe / Wade / Abortion (June)\n",
    "* Shinzo / Abe / Japan(July)\n",
    "* Trump / Mar-a-Lago (August)\n",
    "* Queen / Elizabeth / England (September)\n",
    "* Elon / Musk / Twitter (October)\n",
    "* Republicans / Red / Wave (November)\n",
    "* Russia / Brittney / Griner / Prisoner (December)\n",
    "\n",
    "***2023 (TODO)***\n",
    "* xxx (January)\n",
    "* xxx (February)\n",
    "* xxx (March)\n",
    "* xxx (April)\n",
    "\n",
    "Timeframe: 2022-06-01 to 2023-04-28\n",
    "source: [cbsnews](https://www.cbsnews.com/news/the-year-in-review-top-news-stories-of-2022-month-by-month/)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create Corpora"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [],
   "source": [
    "corpus_all = df_all[\"text\"].values.tolist()\n",
    "\n",
    "corpora = [df[\"text\"].values.tolist() for df in dataframes]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create Tokens"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 599564/599564 [00:00<00:00, 4230558.02it/s]\n",
      "100%|██████████| 8/8 [00:32<00:00,  4.10s/it]\n"
     ]
    }
   ],
   "source": [
    "tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "\n",
    "tokens_all = tokenizer.tokenize(\" \".join(str(text) for text in tqdm(corpus_all)))\n",
    "\n",
    "tokens = [tokenizer.tokenize(\" \".join(str(text) for text in corpus)) for corpus in tqdm(corpora)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create Vocabularies"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 8/8 [00:01<00:00,  7.37it/s]\n"
     ]
    }
   ],
   "source": [
    "# unique vocabulary of each class\n",
    "vocabulary_all = set(tokens_all)\n",
    "\n",
    "vocabularies = [set(tokens_split) for tokens_split in tqdm(tokens)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [],
   "source": [
    "vocabulary_size_all = len(vocabulary_all)\n",
    "\n",
    "vocabulary_sizes = [len(vocabulary) for vocabulary in vocabularies]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 220439\n",
      "Vocabulary size of split 1: 106720\n",
      "Vocabulary size of split 2: 167134\n",
      "Vocabulary size of split 3: 164093\n",
      "Vocabulary size of split 4: 110498\n",
      "Vocabulary size of split 5: 125418\n",
      "Vocabulary size of split 6: 151338\n",
      "Vocabulary size of split 7: 134497\n",
      "Vocabulary size of split 8: 142440\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocabulary size: {vocabulary_size_all}\")\n",
    "counter = 1\n",
    "for vocab_size in vocabulary_sizes:\n",
    "    print(f\"Vocabulary size of split {counter}: {vocab_size}\")\n",
    "    counter += 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Word Embeddings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. Static: Word2Vec (Gensim)\n",
    "\n",
    "2. Temporal: TWEC/CADE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Word2Vec (static)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 599564/599564 [01:00<00:00, 9874.25it/s] \n"
     ]
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "tokenized_sentences_all = [word_tokenize(item) for item in tqdm(corpus_all)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(tokenized_sentences_all, min_count = 30, seed=1040)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "[('trumps', 0.8276152014732361),\n ('donald', 0.6924834251403809),\n ('biden', 0.6235580444335938),\n ('bidens', 0.6000710725784302),\n ('obama', 0.5897787809371948),\n ('presidency', 0.5470191240310669),\n ('desantis', 0.5289232730865479),\n ('impeachment', 0.5264337658882141),\n ('marfia', 0.5250229239463806),\n ('putin', 0.5220085382461548)]"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_word = \"trump\"\n",
    "w2v_model.wv.most_similar(test_word)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/Users/paulschmitt/miniforge3/envs/stream1/lib/python3.8/site-packages/smart_open/smart_open_lib.py:408: UserWarning: This function is deprecated, use smart_open.open instead. See the migration notes for details: https://github.com/RaRe-Technologies/smart_open/blob/master/README.rst#migrating-to-the-new-open-function\n",
      "  warnings.warn(\n"
     ]
    }
   ],
   "source": [
    "w2v_model.save(\"model/word2vec.model\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Temporal: TWEC/CADE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "***Temporal Word Embeddings with a Compass***\n",
    "\n",
    "* [Source-Code](https://github.com/valedica/twec)\n",
    "\n",
    "* [Paper](https://arxiv.org/abs/1906.02376)\n",
    "\n",
    "* [Blogpost](https://fede-bianchi.medium.com/aligning-temporal-diachronic-word-embeddings-with-a-compass-732ab7427955)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Save the concatenated text to txt-files to make them usable for Cade"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "from cade.cade import CADE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [],
   "source": [
    "cade_dir = DIR / \"cade\"\n",
    "cade_split_dir = cade_dir\n",
    "cade_split_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "file_paths = [(cade_split_dir / csv_file.split(\"/\")[-1].split(\".\")[0]).with_suffix(\".txt\") for csv_file in csv_files]\n",
    "\n",
    "file_paths_and_corpora = {\n",
    "    cade_dir / 'compass.txt': corpus_all\n",
    "}\n",
    "\n",
    "for key, value in zip(file_paths, corpora):\n",
    "    file_paths_and_corpora[key] = value\n",
    "\n",
    "for file_path, corpus in file_paths_and_corpora.items():\n",
    "    with open(file_path, 'w') as file:\n",
    "        for item in corpus:\n",
    "            file.write(\"%s\\n\" % item)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "# only needed once for the installation & creation of a venv\n",
    "''''%%capture\n",
    "!pip install -U cade\n",
    "!pip install git+https://github.com/vinid/gensim.git''';"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create & train the compass\n",
    "\n",
    "This creates atemporal context and target word embeddings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the compass from scratch.\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "aligner = CADE(size=30, min_count = 30)\n",
    "aligner.train_compass(str((cade_dir / \"compass\").with_suffix(\".txt\")), overwrite=True);"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [
    {
     "data": {
      "text/plain": "[PosixPath('data/cade/01_Jun_to_01_Oct.txt'),\n PosixPath('data/cade/01_Oct_to_28_Apr.txt'),\n PosixPath('data/cade/07_Oct_to_28_Apr.txt'),\n PosixPath('data/cade/01_Jun_to_07_Oct.txt'),\n PosixPath('data/cade/01_Jun_to_26_Oct.txt'),\n PosixPath('data/cade/26_Oct_to_28_Apr.txt'),\n PosixPath('data/cade/18_Nov_to_28_Apr.txt'),\n PosixPath('data/cade/01_Jun_to_18_Nov.txt')]"
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_paths"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training embeddings: slice data/cade/01_Jun_to_01_Oct.txt.\n",
      "Initializing embeddings from compass.\n",
      "Training embeddings: slice data/cade/01_Oct_to_28_Apr.txt.\n",
      "Initializing embeddings from compass.\n",
      "Training embeddings: slice data/cade/07_Oct_to_28_Apr.txt.\n",
      "Initializing embeddings from compass.\n",
      "Training embeddings: slice data/cade/01_Jun_to_07_Oct.txt.\n",
      "Initializing embeddings from compass.\n",
      "Training embeddings: slice data/cade/01_Jun_to_26_Oct.txt.\n",
      "Initializing embeddings from compass.\n",
      "Training embeddings: slice data/cade/26_Oct_to_28_Apr.txt.\n",
      "Initializing embeddings from compass.\n",
      "Training embeddings: slice data/cade/18_Nov_to_28_Apr.txt.\n",
      "Initializing embeddings from compass.\n",
      "Training embeddings: slice data/cade/01_Jun_to_18_Nov.txt.\n",
      "Initializing embeddings from compass.\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# train slices, they will be already aligned\n",
    "slices = [aligner.train_slice(file_path, save=True) for file_path in file_paths] # list of gensim word2vec objects"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load the models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [],
   "source": [
    "'''from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "model_dir = Path(\"model/\")\n",
    "model_files = glob.glob(str(model_dir / \"*.model\"))\n",
    "\n",
    "models = [Word2Vec.load(model_file) for model_file in model_files]''';"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [],
   "source": [
    "'''\n",
    "model_one = models[0]\n",
    "model_two = models[1]\n",
    "''';"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"done\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
