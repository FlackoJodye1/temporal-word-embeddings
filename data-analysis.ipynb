{
 "cells": [
  {
   "cell_type": "markdown",
   "source": [
    "# Data-Analysis"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import glob\n",
    "import warnings\n",
    "from pathlib import Path\n",
    "from tqdm.notebook import tqdm\n",
    "\n",
    "import nltk"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Read data from processed csv files"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "outputs": [],
   "source": [
    "DIR = Path(\"data\")\n",
    "split_dir = DIR / \"split\"\n",
    "sub_dir = \"monthly\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "outputs": [],
   "source": [
    "if sub_dir:\n",
    "    csv_files = glob.glob(str(split_dir / sub_dir / \"*.csv\"))\n",
    "else:\n",
    "    csv_files = glob.glob(str(split_dir / \"*/*.csv\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "outputs": [],
   "source": [
    "dataframes = []\n",
    "for file in csv_files:\n",
    "    df = pd.read_csv(file)\n",
    "    dataframes.append(df)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "outputs": [],
   "source": [
    "# used for training the Cade-Compass\n",
    "df_all = pd.concat(dataframes, ignore_index=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Imported 11 files\n",
      "Data from Period 1: 2023-03-01 00:00:00 - 2023-03-31 23:56:29\n",
      "Data from Period 2: 2023-01-01 00:00:00 - 2023-01-31 23:58:10\n",
      "Data from Period 3: 2023-02-01 00:00:00 - 2023-02-28 23:59:44\n",
      "Data from Period 4: 2022-06-01 23:00:00 - 2022-06-30 23:52:16\n",
      "Data from Period 5: 2022-09-01 00:05:10 - 2022-09-30 22:58:00\n",
      "Data from Period 6: 2023-04-01 00:04:00 - 2023-04-28 07:44:34\n",
      "Data from Period 7: 2022-10-01 23:01:00 - 2022-10-31 23:59:59\n",
      "Data from Period 8: 2022-08-01 00:00:00 - 2022-08-31 23:54:58\n",
      "Data from Period 9: 2022-12-01 00:00:00 - 2022-12-31 23:42:49\n",
      "Data from Period 10: 2022-11-01 00:00:00 - 2022-11-30 23:58:00\n",
      "Data from Period 11: 2022-07-01 00:00:00 - 2022-07-31 23:59:34\n"
     ]
    }
   ],
   "source": [
    "print(f\"Imported {len(dataframes)} files\")\n",
    "\n",
    "for i, df in enumerate(dataframes):\n",
    "    print(f\"Data from Period {i+1}: {df.date.min()} - {df.date.max()}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "outputs": [],
   "source": [
    "# convert date back to datetime object and sort them by date\n",
    "for df in dataframes:\n",
    "    df['date'] =  pd.to_datetime(df['date'])\n",
    "    df.sort_values('date', inplace=True)\n",
    "\n",
    "df_all['date'] =  pd.to_datetime(df_all['date'])\n",
    "df_all.sort_values('date', inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Check if any missing values are in the (processed) text column"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "outputs": [
    {
     "data": {
      "text/plain": "0"
     },
     "execution_count": 8,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "df_all[\"text\"].isna().sum()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "If there are any but still amount to only an insignificant portion of the data, we delete them as to not cause problems with nltk's tokenizers."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "outputs": [],
   "source": [
    "df_all.dropna(subset=['text'], inplace=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Sample-Engine"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "***CBS-News - The Hottest Topic of each Month:***\n",
    "\n",
    "***2022***\n",
    "* Roe / Wade / Abortion (June)\n",
    "* Shinzo / Abe / Japan(July)\n",
    "* Trump / Mar-a-Lago (August)\n",
    "* Queen / Elizabeth / England (September)\n",
    "* Elon / Musk / Twitter (October)\n",
    "* Republicans / Red / Wave (November)\n",
    "* Russia / Brittney / Griner / Prisoner (December)\n",
    "\n",
    "***2023 (TODO)***\n",
    "* xxx (January)\n",
    "* xxx (February)\n",
    "* xxx (March)\n",
    "* xxx (April)\n",
    "\n",
    "Timeframe: 2022-06-01 to 2023-04-28\n",
    "source: [cbsnews](https://www.cbsnews.com/news/the-year-in-review-top-news-stories-of-2022-month-by-month/)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "outputs": [],
   "source": [
    "def sample_text(df, month = \"06\"):\n",
    "    sample_stream_1 = df.sample(1)\n",
    "    print(\"Stream 1\")\n",
    "    print(f\"Date: {sample_stream_1['date'].iloc[0].date()}\")\n",
    "    print(f\"Sentiment: {sample_stream_1['sentiment'].iloc[0]}\")\n",
    "    print(\"----------------\")\n",
    "    print(sample_stream_1[\"text\"].iloc[0])"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Notable Events:\n",
    "\n",
    "* Teacherstrike"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Stream 1\n",
      "Date: 2023-01-23\n",
      "Sentiment: Negatives\n",
      "----------------\n",
      "additionally gen z grew up in the digital age the first generation to only know a world with the internet however despite the infinite connectivity few members of gen z have the communication and interpersonal skills deemed necessary for successful careers much of this is due to the covid19 pandemic which forced gen z students to move their studies online hindering their abilities to foster formal and informal inperson interactions according to the workforce institute 34 of gen z americans blame educational barriers for their lack of skillsbased knowledge in the professional world 2 the pandemic also marked significant socioeconomic disparities from race to gender\n"
     ]
    }
   ],
   "source": [
    "sample_text(df_all)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create Corpora"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "outputs": [],
   "source": [
    "corpus_all = df_all[\"text\"].values.tolist()\n",
    "\n",
    "corpora = [df[\"text\"].values.tolist() for df in dataframes]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create Tokens"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/149998 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "7a48415b43fd4ffbb62d4e470c3ecb65"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": "  0%|          | 0/11 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "788e6899935c4aac817fdf9a3edca49c"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "tokenizer = nltk.tokenize.TreebankWordTokenizer()\n",
    "\n",
    "tokens_all = tokenizer.tokenize(\" \".join(str(text) for text in tqdm(corpus_all)))\n",
    "\n",
    "tokens = [tokenizer.tokenize(\" \".join(str(text) for text in corpus)) for corpus in tqdm(corpora)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Create Vocabularies"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/11 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "2cd2f5536149459989cd792bf4bcf06d"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# unique vocabulary of each class\n",
    "vocabulary_all = set(tokens_all)\n",
    "\n",
    "vocabularies = [set(tokens_split) for tokens_split in tqdm(tokens)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "outputs": [],
   "source": [
    "vocabulary_size_all = len(vocabulary_all)\n",
    "\n",
    "vocabulary_sizes = [len(vocabulary) for vocabulary in vocabularies]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "outputs": [
    {
     "data": {
      "text/plain": "1"
     },
     "execution_count": 16,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "tokens[0].count(\"pelosis\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Vocabulary size: 220582\n",
      "Vocabulary size of split 1: 47890\n",
      "Vocabulary size of split 2: 49578\n",
      "Vocabulary size of split 3: 43668\n",
      "Vocabulary size of split 4: 41172\n",
      "Vocabulary size of split 5: 43809\n",
      "Vocabulary size of split 6: 40843\n",
      "Vocabulary size of split 7: 55486\n",
      "Vocabulary size of split 8: 47271\n",
      "Vocabulary size of split 9: 49526\n",
      "Vocabulary size of split 10: 53728\n",
      "Vocabulary size of split 11: 47945\n"
     ]
    }
   ],
   "source": [
    "print(f\"Vocabulary size: {vocabulary_size_all}\")\n",
    "for i, vocab_size in enumerate(vocabulary_sizes):\n",
    "    print(f\"Vocabulary size of split {i+1}: {vocab_size}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Word Embeddings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "1. Static: Word2Vec (Gensim)\n",
    "\n",
    "2. Temporal: TWEC/CADE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 1. Word2Vec (static)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/149998 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "155939ebc7af4b2295abbeeac60c3be9"
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "from nltk import word_tokenize\n",
    "\n",
    "tokenized_sentences_all = [word_tokenize(item) for item in tqdm(corpus_all)]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "w2v_model = Word2Vec(tokenized_sentences_all, min_count = 1, seed=1040)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [
    {
     "data": {
      "text/plain": "[('donald', 0.7860180735588074),\n ('biden', 0.7639197111129761),\n ('trumps', 0.7628220319747925),\n ('obama', 0.6877856254577637),\n ('joe', 0.6639670729637146),\n ('thenpresident', 0.6630730628967285),\n ('putin', 0.6451755166053772),\n ('desantis', 0.6334975957870483),\n ('crist', 0.6087217330932617),\n ('bidens', 0.6021501421928406)]"
     },
     "execution_count": 21,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_word = \"trump\"\n",
    "w2v_model.wv.most_similar(test_word)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [],
   "source": [
    "w2v_model.save(\"model/static/word2vec.model\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## 2. Temporal: TWEC/CADE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "***Temporal Word Embeddings with a Compass***\n",
    "\n",
    "* [Source-Code](https://github.com/valedica/twec)\n",
    "\n",
    "* [Paper](https://arxiv.org/abs/1906.02376)\n",
    "\n",
    "* [Blogpost](https://fede-bianchi.medium.com/aligning-temporal-diachronic-word-embeddings-with-a-compass-732ab7427955)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Save the concatenated text to txt-files to make them usable for Cade"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "outputs": [],
   "source": [
    "from cade.cade import CADE"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "outputs": [],
   "source": [
    "cade_dir = DIR / \"cade\"\n",
    "cade_split_dir = cade_dir / sub_dir\n",
    "cade_split_dir.mkdir(parents=True, exist_ok=True)\n",
    "\n",
    "file_paths = [(cade_split_dir / csv_file.split(\"/\")[-1].split(\".\")[0]).with_suffix(\".txt\") for csv_file in csv_files]\n",
    "\n",
    "file_paths_and_corpora = {\n",
    "    cade_dir / 'compass.txt': corpus_all\n",
    "}\n",
    "\n",
    "for key, value in zip(file_paths, corpora):\n",
    "    file_paths_and_corpora[key] = value\n",
    "\n",
    "for file_path, corpus in file_paths_and_corpora.items():\n",
    "    with open(file_path, 'w') as file:\n",
    "        for item in corpus:\n",
    "            file.write(\"%s\\n\" % item)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "outputs": [],
   "source": [
    "# only needed once for the installation & creation of a venv\n",
    "'''%%capture\n",
    "!pip install -U cade\n",
    "!pip install git+https://github.com/vinid/gensim.git''';"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create & train the compass\n",
    "\n",
    "This creates atemporal context and target word embeddings"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training the compass from scratch.\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "aligner = CADE(size=30, min_count = 1)\n",
    "aligner.train_compass(str((cade_dir / \"compass\").with_suffix(\".txt\")), overwrite=True);"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "outputs": [
    {
     "data": {
      "text/plain": "[PosixPath('data/cade/monthly/01_Mar_to_01_Apr.txt'),\n PosixPath('data/cade/monthly/01_Jan_to_01_Feb.txt'),\n PosixPath('data/cade/monthly/01_Feb_to_01_Mar.txt'),\n PosixPath('data/cade/monthly/01_Jun_to_01_Jul.txt'),\n PosixPath('data/cade/monthly/01_Sep_to_01_Oct.txt'),\n PosixPath('data/cade/monthly/01_Apr_to_28_Apr.txt'),\n PosixPath('data/cade/monthly/01_Oct_to_01_Nov.txt'),\n PosixPath('data/cade/monthly/01_Aug_to_01_Sep.txt'),\n PosixPath('data/cade/monthly/01_Dec_to_01_Jan.txt'),\n PosixPath('data/cade/monthly/01_Nov_to_01_Dec.txt'),\n PosixPath('data/cade/monthly/01_Jul_to_01_Aug.txt')]"
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "file_paths"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training embeddings: slice data/cade/monthly/01_Mar_to_01_Apr.txt.\n",
      "Initializing embeddings from compass.\n",
      "Training embeddings: slice data/cade/monthly/01_Jan_to_01_Feb.txt.\n",
      "Initializing embeddings from compass.\n",
      "Training embeddings: slice data/cade/monthly/01_Feb_to_01_Mar.txt.\n",
      "Initializing embeddings from compass.\n",
      "Training embeddings: slice data/cade/monthly/01_Jun_to_01_Jul.txt.\n",
      "Initializing embeddings from compass.\n",
      "Training embeddings: slice data/cade/monthly/01_Sep_to_01_Oct.txt.\n",
      "Initializing embeddings from compass.\n",
      "Training embeddings: slice data/cade/monthly/01_Apr_to_28_Apr.txt.\n",
      "Initializing embeddings from compass.\n",
      "Training embeddings: slice data/cade/monthly/01_Oct_to_01_Nov.txt.\n",
      "Initializing embeddings from compass.\n",
      "Training embeddings: slice data/cade/monthly/01_Aug_to_01_Sep.txt.\n",
      "Initializing embeddings from compass.\n",
      "Training embeddings: slice data/cade/monthly/01_Dec_to_01_Jan.txt.\n",
      "Initializing embeddings from compass.\n",
      "Training embeddings: slice data/cade/monthly/01_Nov_to_01_Dec.txt.\n",
      "Initializing embeddings from compass.\n",
      "Training embeddings: slice data/cade/monthly/01_Jul_to_01_Aug.txt.\n",
      "Initializing embeddings from compass.\n"
     ]
    }
   ],
   "source": [
    "warnings.filterwarnings(\"ignore\")\n",
    "\n",
    "# train slices, they will be already aligned\n",
    "slices = [aligner.train_slice(file_path, save=True) for file_path in file_paths] # list of gensim word2vec objects"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Load the models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "done\n"
     ]
    }
   ],
   "source": [
    "print(\"done\")"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
