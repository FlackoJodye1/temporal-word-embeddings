{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {
    "collapsed": true
   },
   "outputs": [],
   "source": [
    "import json\n",
    "import pickle\n",
    "import pandas as pd\n",
    "from glob import glob\n",
    "from pathlib import Path\n",
    "from scipy import spatial\n",
    "import scipy.sparse as sp\n",
    "from itertools import islice\n",
    "from tqdm.notebook import tqdm\n",
    "from gensim.models.word2vec import Word2Vec\n",
    "\n",
    "import sys\n",
    "sys.path.append('../../')\n",
    "\n",
    "from src.packages.TPPMI.ppmi_model import PPMIModel\n",
    "from src.packages.TPPMI.tppmi_model import TPPMIModel"
   ]
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Imports"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 190,
   "outputs": [],
   "source": [
    "path_to_test_data = Path(\"../../data\") / \"test\"\n",
    "path_to_tppmi_model = Path(\"../../data\") / \"ppmi-matrices\" / \"nyt-data\"\n",
    "path_to_twec_model = Path(\"../../model\") / \"nyt-data\" / \"cade\" / \"model\"\n",
    "path_to_static_model = Path(\"../../model\") / \"nyt-data\" / \"static\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Functions\n",
    "\n",
    "(to be outsourced later)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Similarity Calculations"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 246,
   "outputs": [],
   "source": [
    "def get_similarities_of_models(model_dict: dict, test_word_dict: dict) -> dict:\n",
    "    similarities = dict()\n",
    "    for test_word in tqdm(test_word_dict.items()):\n",
    "        similarities[test_word[0]] = dict()\n",
    "        for model in model_dict.items():\n",
    "            similarities[test_word[0]][model[0].split(\"_\")[1]] = model[1].wv.similar_by_vector(test_word[1])\n",
    "    return similarities"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 249,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/497 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "ecf19216919f4a2dac5434e15d201de1"
      },
      "application/json": {
       "n": 0,
       "total": 497,
       "elapsed": 0.0037381649017333984,
       "ncols": null,
       "nrows": null,
       "prefix": "",
       "ascii": false,
       "unit": "it",
       "unit_scale": false,
       "rate": null,
       "bar_format": null,
       "postfix": null,
       "unit_divisor": 1000,
       "initial": 0,
       "colour": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "similarities_test = get_similarities_of_models(cade_models, test_case_dict_1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Evaluation Metrics"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 250,
   "outputs": [],
   "source": [
    "def calculate_reciprocal_rank(test_list: list, test_word: str) -> float:\n",
    "    \"\"\"\n",
    "    Calculate the reciprocal rank for a given test word in a list of strings.\n",
    "\n",
    "    Parameters:\n",
    "    test_list (list of str): The list of strings to search through.\n",
    "    test_word (str): The correct answer to find in the test_list.\n",
    "    Returns:\n",
    "    float: The reciprocal rank of the test_word in test_list, or 0 if not found.\n",
    "    \"\"\"\n",
    "    try:\n",
    "        rank = test_list.index(test_word) + 1  # Adding 1 because index is 0-based and rank is 1-based\n",
    "        return 1.0 / rank\n",
    "    except ValueError:\n",
    "        return 0.0  # test_word not found in test_list"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 251,
   "outputs": [],
   "source": [
    "def calculate_precision_at_k(test_list: list, test_word: str, k: int) -> int:\n",
    "    \"\"\"\n",
    "    Calculate the precision at K for a given test word in a list of strings.\n",
    "\n",
    "    Parameters:\n",
    "    test_list (list of str): The list of strings to search through, assumed to be ordered by relevance.\n",
    "    test_word (str): The correct answer to find in the test_list.\n",
    "    k (int): The number of top items to consider for calculating precision.\n",
    "\n",
    "    Returns:\n",
    "    int: The precision at K for the test_word in test_list.\n",
    "         If the target word is among these K words, then the Precision@K for test i\n",
    "         (denoted P@K[i]) is 1; else, it is 0\n",
    "    \"\"\"\n",
    "    if k <= 0:\n",
    "        raise ValueError(\"k must be a positive integer\")\n",
    "\n",
    "    # Take the top K elements from the list\n",
    "    top_k = test_list[:k]\n",
    "\n",
    "    # Check if the test_word is within the top K elements\n",
    "    if test_word in top_k:\n",
    "        return 1\n",
    "    else:\n",
    "        return 0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 252,
   "outputs": [],
   "source": [
    "def calculate_mean_rank(test_key: str, testcase: dict, test_data: pd.DataFrame, metric = \"MRR\", k = 10)-> float:\n",
    "\n",
    "    test_data_for_key = test_data[test_data[\"truth\"] == test_key]\n",
    "    ranks = []\n",
    "\n",
    "    for key, value in testcase.items():\n",
    "        test_data_for_year = test_data_for_key[test_data_for_key[\"equivalent\"].str.endswith(key)]\n",
    "        word_list = [item[0] for item in value]\n",
    "\n",
    "        if len(test_data_for_year) == 0:\n",
    "            continue  # Skip if no data for year, as there's nothing to rank\n",
    "        target_word = test_data_for_year[\"equivalent\"].iloc[0].split(\"-\")[0]\n",
    "        if metric == \"MRR\":\n",
    "            rank = calculate_reciprocal_rank(word_list, target_word)\n",
    "        else:\n",
    "            rank = calculate_precision_at_k(word_list, target_word, k)\n",
    "\n",
    "        ranks.append(rank)\n",
    "\n",
    "    if ranks:  # Ensure division by 0 does not occur\n",
    "        mean_rank = sum(ranks) / len(ranks)\n",
    "    else:\n",
    "        mean_rank = 0\n",
    "\n",
    "    return mean_rank"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 253,
   "outputs": [],
   "source": [
    "def calculate_rank_metric(similarities: dict, test_data: pd.DataFrame, metric = \"MRR\", k = 10) -> float:\n",
    "    ranks = []\n",
    "    for key, value in similarities.items():\n",
    "        rank = calculate_mean_rank(key, value, test_data, metric, k)\n",
    "\n",
    "        ranks.append(rank)\n",
    "\n",
    "    if ranks:  # Ensure division by 0 does not occur\n",
    "        mean_rank = sum(ranks) / len(ranks)\n",
    "    else:\n",
    "        mean_rank = 0\n",
    "\n",
    "    return mean_rank"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Setup"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Testsets"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Testset 1\n",
    "\n",
    "Based on publicly recorded knowledge that for each year lists different names for a particular role, such as U.S. president, U.K. prime minister, NFL superbowl champion team, and so on."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 254,
   "outputs": [],
   "source": [
    "test_data_1 = pd.read_csv(path_to_test_data / \"testset_1.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 255,
   "outputs": [],
   "source": [
    "test_data_1.columns = ['truth', 'equivalent']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 256,
   "outputs": [],
   "source": [
    "test_cases_1 = test_data_1['truth'].unique()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 257,
   "outputs": [],
   "source": [
    "test_data_1 = test_data_1.sort_values(by='truth', ascending=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 258,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testset 1\n",
      "Testcases (all): 11027\n",
      "Testcases (unique): 499\n"
     ]
    }
   ],
   "source": [
    "print(\"Testset 1\")\n",
    "print(f\"Testcases (all): {len(test_data_1)}\")\n",
    "print(f\"Testcases (unique): {len(test_cases_1)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Now we want to split the testset into static & dynamic testcases as was done by Di Carlo et al. in their paper \"Training Temporal Word Embeddings with a Compass\""
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 259,
   "outputs": [],
   "source": [
    "# Splitting the columns into words and years\n",
    "split_truth = test_data_1['truth'].str.split('-', expand=True)\n",
    "split_equivalent = test_data_1['equivalent'].str.split('-', expand=True)\n",
    "\n",
    "# Creating masks for \"static\" and \"dynamic\" conditions\n",
    "static_mask = split_truth[0] == split_equivalent[0]\n",
    "dynamic_mask = split_truth[0] != split_equivalent[0]\n",
    "\n",
    "# Applying the masks to create the separate DataFrames\n",
    "static_df = test_data_1[static_mask]\n",
    "dynamic_df = test_data_1[dynamic_mask]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 260,
   "outputs": [],
   "source": [
    "static_test_cases = static_df['truth'].unique()\n",
    "dynamic_test_cases = dynamic_df['truth'].unique()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 261,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Static\n",
      "Testcases (all): 2937\n",
      "Testcases (unique): 443\n"
     ]
    }
   ],
   "source": [
    "print(\"Static\")\n",
    "print(f\"Testcases (all): {len(static_df)}\")\n",
    "print(f\"Testcases (unique): {len(static_test_cases)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 262,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Dynamic\n",
      "Testcases (all): 8090\n",
      "Testcases (unique): 499\n"
     ]
    }
   ],
   "source": [
    "print(\"Dynamic\")\n",
    "print(f\"Testcases (all): {len(dynamic_df)}\")\n",
    "print(f\"Testcases (unique): {len(dynamic_test_cases)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Testset 2\n",
    "\n",
    "Testset 2 is human-generated, for exploring more interesting concepts like emerging technologies, brands and major events (e.g., disease outbreaks and financial crisis). For constructing the test word pairs, we first select emerging terms which have not been popularized before 1994, then query their well known precedents during 1990 to 1994 (e.g., app-2012 can correspond to software-1990)."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 263,
   "outputs": [],
   "source": [
    "test_data_2 = pd.read_csv(path_to_test_data / \"testset_2.csv\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 264,
   "outputs": [],
   "source": [
    "test_data_2.columns = ['truth', 'equivalent']"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 265,
   "outputs": [],
   "source": [
    "test_cases_2 = test_data_2['truth'].unique()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 266,
   "outputs": [],
   "source": [
    "test_data_2 = test_data_2.sort_values(by='truth', ascending=True)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 267,
   "outputs": [
    {
     "data": {
      "text/plain": "             truth    equivalent\n57  amazoncom-2000  walmart-1993\n63  amazoncom-2000     macy-1994\n62  amazoncom-2000     macy-1993\n61  amazoncom-2000     macy-1992\n60  amazoncom-2000     macy-1991",
      "text/html": "<div>\n<style scoped>\n    .dataframe tbody tr th:only-of-type {\n        vertical-align: middle;\n    }\n\n    .dataframe tbody tr th {\n        vertical-align: top;\n    }\n\n    .dataframe thead th {\n        text-align: right;\n    }\n</style>\n<table border=\"1\" class=\"dataframe\">\n  <thead>\n    <tr style=\"text-align: right;\">\n      <th></th>\n      <th>truth</th>\n      <th>equivalent</th>\n    </tr>\n  </thead>\n  <tbody>\n    <tr>\n      <th>57</th>\n      <td>amazoncom-2000</td>\n      <td>walmart-1993</td>\n    </tr>\n    <tr>\n      <th>63</th>\n      <td>amazoncom-2000</td>\n      <td>macy-1994</td>\n    </tr>\n    <tr>\n      <th>62</th>\n      <td>amazoncom-2000</td>\n      <td>macy-1993</td>\n    </tr>\n    <tr>\n      <th>61</th>\n      <td>amazoncom-2000</td>\n      <td>macy-1992</td>\n    </tr>\n    <tr>\n      <th>60</th>\n      <td>amazoncom-2000</td>\n      <td>macy-1991</td>\n    </tr>\n  </tbody>\n</table>\n</div>"
     },
     "execution_count": 267,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "test_data_2.head()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 268,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Testset 2\n",
      "Testcases (all): 444\n",
      "Testcases (unique): 46\n"
     ]
    }
   ],
   "source": [
    "print(\"Testset 2\")\n",
    "print(f\"Testcases (all): {len(test_data_2)}\")\n",
    "print(f\"Testcases (unique): {len(test_cases_2)}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Models"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TWEC"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 269,
   "outputs": [],
   "source": [
    "cade_model_filenames = glob(str(path_to_twec_model / \"*.model\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 270,
   "outputs": [
    {
     "data": {
      "text/plain": "  0%|          | 0/27 [00:00<?, ?it/s]",
      "application/vnd.jupyter.widget-view+json": {
       "version_major": 2,
       "version_minor": 0,
       "model_id": "fa6ccea7de6e4db7a924b61549e15f37"
      },
      "application/json": {
       "n": 0,
       "total": 27,
       "elapsed": 0.0018620491027832031,
       "ncols": null,
       "nrows": null,
       "prefix": "",
       "ascii": false,
       "unit": "it",
       "unit_scale": false,
       "rate": null,
       "bar_format": null,
       "postfix": null,
       "unit_divisor": 1000,
       "initial": 0,
       "colour": null
      }
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "# load models\n",
    "cade_models = {f\"model_{model_file.split('_data')[0][-4:]}\":Word2Vec.load(model_file) for model_file in tqdm(cade_model_filenames)}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 271,
   "outputs": [],
   "source": [
    "cade_models = {model_key: cade_models[model_key] for model_key in sorted(cade_models, key=lambda x: int(x.split('_')[1]))}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 272,
   "outputs": [
    {
     "data": {
      "text/plain": "dict_keys(['model_1990', 'model_1991', 'model_1992', 'model_1993', 'model_1994', 'model_1995', 'model_1996', 'model_1997', 'model_1998', 'model_1999', 'model_2000', 'model_2001', 'model_2002', 'model_2003', 'model_2004', 'model_2005', 'model_2006', 'model_2007', 'model_2008', 'model_2009', 'model_2010', 'model_2011', 'model_2012', 'model_2013', 'model_2014', 'model_2015', 'model_2016'])"
     },
     "execution_count": 272,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "cade_models.keys()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "#### Testset 1"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 273,
   "outputs": [],
   "source": [
    "test_case_dict_1 = dict()\n",
    "counter = 0"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 274,
   "outputs": [
    {
     "data": {
      "text/plain": "499"
     },
     "execution_count": 274,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "len(test_cases_1)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 275,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "2 Testcases are not in the vocab of the model(s)\n"
     ]
    }
   ],
   "source": [
    "for test_case in test_cases_1:\n",
    "    word, year = test_case.split(\"-\")\n",
    "    ground_model = cade_models[f\"model_{year}\"]\n",
    "    if word in ground_model.wv.vocab:\n",
    "        test_case_dict_1[test_case] = ground_model.wv.get_vector(word)\n",
    "    else:\n",
    "        counter = counter + 1\n",
    "print(f\"{counter} Testcases are not in the vocab of the model(s)\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 276,
   "outputs": [],
   "source": [
    "cade_model = cade_models[next(iter(cade_models))]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 277,
   "outputs": [],
   "source": [
    "'''\n",
    "# Takes long to execute, load from memory in next-cell\n",
    "\n",
    "cade_similarities = get_similarities_of_models(cade_models, test_case_dict_1)\n",
    "with open(path_to_test_data / 'cade_t1.json', 'w') as f:\n",
    "    json.dump( cade_similarities, f, indent=4)''';"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 278,
   "outputs": [],
   "source": [
    "with open(path_to_test_data / 'cade_t1.json', 'r') as json_file:\n",
    "    cade_similarities = json.load(json_file)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### Static Word2Vec"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 279,
   "outputs": [],
   "source": [
    "from gensim.models import Word2Vec\n",
    "static_model = Word2Vec.load(str(path_to_static_model / \"w2v_model.model\"))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "### TPPMI"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 280,
   "outputs": [],
   "source": [
    "ppmi_data_files = sorted(glob(str(path_to_tppmi_model  / \"*.npz\")))\n",
    "words_files = sorted(glob(str(path_to_tppmi_model  / \"*.pkl\")))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Split context-words from timestamped-vocabularies"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 281,
   "outputs": [],
   "source": [
    "context_words_file = [path for path in words_files if \"context-words\" in path]\n",
    "ppmi_vocab_files = [path for path in words_files if \"context-words\" not in path]"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 282,
   "outputs": [],
   "source": [
    "# Get ppmi-matrices and vocab\n",
    "ppmi_matrices = {}\n",
    "\n",
    "for filenames in zip(ppmi_vocab_files, ppmi_data_files):\n",
    "    ppmi_matrix = sp.load_npz(filenames[1])\n",
    "    with open(filenames[0], \"rb\") as f:\n",
    "        vocab = pickle.load(f)\n",
    "    key = filenames[0].split(\"ppmi-\")[2][0:4]\n",
    "    ppmi_matrices[key] = {\"ppmi_matrix\" : ppmi_matrix, \"vocab\": vocab}\n",
    "\n",
    "# Get common context-words\n",
    "with open(context_words_file[0], \"rb\") as f:\n",
    "    context_words = pickle.load(f)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 283,
   "outputs": [
    {
     "data": {
      "text/plain": "dict_keys(['1990', '1991', '1992', '1993', '1994', '1995', '1996', '1997', '1998', '1999', '2000', '2001', '2002', '2003', '2004', '2005', '2006', '2007', '2008', '2009', '2010', '2011', '2012', '2013', '2014', '2015', '2016'])"
     },
     "execution_count": 283,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "ppmi_matrices.keys()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "Create ppmi_model objects"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 284,
   "outputs": [],
   "source": [
    "ppmi_models = {key: PPMIModel.construct_from_data(ppmi_data[\"ppmi_matrix\"], ppmi_data[\"vocab\"], context_words) for key, ppmi_data in ppmi_matrices.items()}"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 285,
   "outputs": [],
   "source": [
    "tppmi_model = TPPMIModel(ppmi_models, dates=\"years\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 286,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "        truth    equivalent\n",
      "0   bush-1990  clinton-1992\n",
      "1   bush-1990  clinton-1993\n",
      "2   bush-1990  clinton-1994\n",
      "3   bush-1990  clinton-1995\n",
      "4   bush-1990  clinton-1996\n",
      "5   bush-1990  clinton-1997\n",
      "6   bush-1990  clinton-1998\n",
      "7   bush-1990  clinton-1999\n",
      "8   bush-1990     bush-2000\n",
      "9   bush-1990     bush-2001\n",
      "10  bush-1990     bush-2002\n",
      "11  bush-1990     bush-2003\n",
      "12  bush-1990     bush-2004\n",
      "13  bush-1990     bush-2005\n",
      "14  bush-1990     bush-2006\n",
      "15  bush-1990     bush-2007\n",
      "16  bush-1990    obama-2008\n",
      "17  bush-1990    obama-2009\n",
      "18  bush-1990    obama-2010\n",
      "19  bush-1990    obama-2011\n",
      "20  bush-1990    obama-2012\n",
      "21  bush-1990    obama-2013\n",
      "22  bush-1990    obama-2014\n",
      "23  bush-1990    obama-2015\n",
      "24  bush-1990    obama-2016\n"
     ]
    }
   ],
   "source": [
    "test_list = test_data_1[test_data_1.truth == \"bush-1990\"].copy()\n",
    "test_list['year'] = test_list['equivalent'].apply(lambda x: int(x.split('-')[1]))  # Extract year and convert to int\n",
    "test_list = test_list.sort_values(by='year')  # Sort by the new 'year' column\n",
    "test_list = test_list.drop('year', axis=1)\n",
    "print(test_list)"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Experiment"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "To examine the quality of embedding alignment, we create a task to query equivalences across years.\n",
    "\n",
    "For example, given obama-2012, we want to query its equivalent word in 2002. As we know obama is the U.S. president in 2012; its equivalent in 2002 is bush, who was the U.S. president at that time. In this way, we create two testsets.\n",
    "\n",
    "All results are rounded to three decimal places."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 287,
   "outputs": [],
   "source": [
    "cade_scores = dict()"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Mean Reciprocal Rank (@10)\n",
    "\n",
    "The Mean Reciprocal Rank (MRR) is a statistical measure used to evaluate the performance of a system that returns a ranked list of responses to queries. It is the average of the reciprocal ranks of the first correct answer for each query, where the reciprocal rank is the inverse of the rank at which the first relevant answer is found.\n",
    "It is evaluated @10"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 290,
   "outputs": [],
   "source": [
    "cade_similarities = similarities_test"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 291,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MRR of the Cade Model on Testset1: 0.334\n"
     ]
    }
   ],
   "source": [
    "mrr_at_10 = round(calculate_rank_metric(cade_similarities, test_data_1, metric='MRR', k=1), 3)\n",
    "cade_scores[\"mrr@10\"] = mrr_at_10\n",
    "print(f\"MRR of the Cade Model on Testset1: {mrr_at_10}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "## Mean Precision (@K)\n",
    "\n",
    "As introduced by Yao et al(2018) the MP@K is defined as such: consider the K words most similar to the query embedding for the given year. The Precision@K for a particular test i, represented as P@K[i], equals 1 if the target word appears within this set of K words; otherwise, it assumes a value of 0."
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 292,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MP@1 of the Cade Model on Testset1: 0.254\n"
     ]
    }
   ],
   "source": [
    "mp_at_1 = round(calculate_rank_metric(cade_similarities, test_data_1, metric='MP', k=1), 3)\n",
    "cade_scores[\"mp@1\"] = mp_at_1\n",
    "print(f\"MP@1 of the Cade Model on Testset1: {mp_at_1}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 293,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MP@3 of the Cade Model on Testset1: 0.387\n"
     ]
    }
   ],
   "source": [
    "mp_at_3 = round(calculate_rank_metric(cade_similarities, test_data_1, metric='MP', k=3), 3)\n",
    "cade_scores[\"mp@3\"] = mp_at_3\n",
    "print(f\"MP@3 of the Cade Model on Testset1: {mp_at_3}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 294,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MP@5 of the Cade Model on Testset1: 0.443\n"
     ]
    }
   ],
   "source": [
    "mp_at_5 = round(calculate_rank_metric(cade_similarities, test_data_1, metric='MP', k=5), 3)\n",
    "cade_scores[\"mp@5\"] = mp_at_5\n",
    "print(f\"MP@5 of the Cade Model on Testset1: {mp_at_5}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 295,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "MP@5 of the Cade Model on Testset1: 0.505\n"
     ]
    }
   ],
   "source": [
    "mp_at_10 = round(calculate_rank_metric(cade_similarities, test_data_1, metric='MP', k=10), 3)\n",
    "cade_scores[\"mp@10\"] = mp_at_10\n",
    "print(f\"MP@5 of the Cade Model on Testset1: {mp_at_10}\")"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 296,
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{\n",
      "    \"mrr@10\": 0.334,\n",
      "    \"mp@1\": 0.254,\n",
      "    \"mp@3\": 0.387,\n",
      "    \"mp@5\": 0.443,\n",
      "    \"mp@10\": 0.505\n",
      "}\n"
     ]
    }
   ],
   "source": [
    "print(json.dumps(cade_scores, indent=4))"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "markdown",
   "source": [
    "# Scrapyard"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 201,
   "outputs": [],
   "source": [
    "def calculate_mean_reciprocal_rank_test(test_key: str, testcase: dict, test_data: pd.DataFrame):\n",
    "    print(test_key)\n",
    "    test_data_for_key = test_data[test_data[\"truth\"] == test_key]\n",
    "    print(\"test_data_for_key\")\n",
    "    print(f\"length: {len(test_data_for_key)}\")\n",
    "    print(test_data_for_key)\n",
    "    ranks = []\n",
    "    for key, value in testcase.items():\n",
    "        print(f\"Key: {key}\")\n",
    "        test_data_for_year = test_data_for_key[test_data_for_key[\"equivalent\"].str.endswith(key)]\n",
    "        word_list = [item[0] for item in value]\n",
    "        print(\"WORD-LIST\")\n",
    "        print(word_list)\n",
    "        if len(test_data_for_year) == 0:\n",
    "            print(\"CONTINUE\")\n",
    "            continue # this means that it is the same year as the word we want to test --> no need to calculate\n",
    "        print(\"test_data_for_year\")\n",
    "        print(test_data_for_year)\n",
    "        print(test_data_for_year[\"equivalent\"].iloc[0].split(\"-\")[0])\n",
    "        reciprocal_rank = calculate_reciprocal_rank(word_list, test_data_for_year[\"equivalent\"].iloc[0].split(\"-\")[0])\n",
    "        print(reciprocal_rank)\n",
    "        ranks.append(reciprocal_rank)\n",
    "\n",
    "    if ranks:  # Ensure division by zero does not occur\n",
    "        mrr = sum(ranks) / len(ranks)\n",
    "    else:\n",
    "        mrr = 0\n",
    "    return mrr"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": 141,
   "outputs": [],
   "source": [
    "def calculate_mrr_for_key(data_dict, key):\n",
    "    \"\"\"\n",
    "    Calculate the Mean Reciprocal Rank (MRR) for a given key in the data dictionary.\n",
    "\n",
    "    Parameters:\n",
    "    data_dict (dict): The dictionary containing years as keys and lists of word-score pairs as values.\n",
    "    key (str): The key in the dictionary to calculate MRR for. Assumes format 'word-year'.\n",
    "\n",
    "    Returns:\n",
    "    float: The MRR for the given key.\n",
    "    \"\"\"\n",
    "    test_word = key.split('-')[0]  # Assuming the \"test word\" is the part of the key before the hyphen\n",
    "    total_reciprocal_rank = 0\n",
    "    num_years = 0\n",
    "\n",
    "    for year, word_score_pairs in data_dict[key].items():\n",
    "        for rank, (word, score) in enumerate(word_score_pairs, start=1):\n",
    "            if word == test_word:\n",
    "                total_reciprocal_rank += 1.0 / rank\n",
    "                break  # Stop looking once the first instance of the test word is found\n",
    "        num_years += 1\n",
    "\n",
    "    # Calculate MRR\n",
    "    if num_years > 0:\n",
    "        return total_reciprocal_rank / num_years\n",
    "    else:\n",
    "        return 0.0  # Return 0 if there are no years/data to calculate MRR"
   ],
   "metadata": {
    "collapsed": false
   }
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "outputs": [],
   "source": [
    "def get_similarities_of_model_manual(model, test_word, top_n = 10) -> list:\n",
    "    # Compute cosine similarity between specified embedding and all embeddings in the model\n",
    "    test_word_embedding = test_word[1]\n",
    "    word_similarities = dict()\n",
    "    for reference_word in model.wv.vocab:\n",
    "        reference_word_embedding = model.wv[reference_word]\n",
    "        similarity = 1 - spatial.distance.cosine(test_word_embedding, reference_word_embedding)\n",
    "        word_similarities[reference_word] = similarity\n",
    "\n",
    "    # Sort words by similarity\n",
    "    sorted_similarities = sorted(word_similarities.items(), key=lambda item: item[1], reverse=True)\n",
    "\n",
    "    print(\"Sorted Similarities\")\n",
    "    print(sorted_similarities[:top_n])\n",
    "\n",
    "    # Get top_n similar words\n",
    "    return sorted_similarities[:top_n]"
   ],
   "metadata": {
    "collapsed": false
   }
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 2
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython2",
   "version": "2.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 0
}
